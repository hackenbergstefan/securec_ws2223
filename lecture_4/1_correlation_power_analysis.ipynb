{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Power Analysis (Brier et al. 2004)\n",
    "\n",
    "\n",
    "#### Learning goals\n",
    "- Learn the mathematical backgrounds of _Correlation Power Analysis_\n",
    "- Learn how to perform a Correlation Power Analysis\n",
    "- Learn how to perform a Correlation Power Analysis using Lascar\n",
    "\n",
    "#### References\n",
    "- [E. Brier, C. Clavier, and F. Olivier. Correlation Power Analysis with a Leakage Model. In M. Joye and J.-J. Quisquater, editors, Cryptographic Hardware and Embedded Systems – CHES 2004, volume 3156 of Lecture Notes in Computer Science, pages 16–29. Springer, 2004](https://www.iacr.org/archive/ches2004/31560016/31560016.pdf)\n",
    "- J. van Woudenberg, C. O'Flynn. The Hardware Hacking Handbook - Chapter \"Correlation Power Analysis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "The DPA attack assumes that for a particular device, you'll get a difference in power consumption when a bit is a 1 or a 0.\n",
    "Any of the 8 bits of one byte is suitable to mount an attack on the AES SBox output.\n",
    "This redundancy is something we can actually use to strengthen our attack.\n",
    "\n",
    "The basic of DPA is: \"If some intermediate bit varies, the power consumption varies with it.\"\n",
    "This does not cover the full extent of the relationship between data and power consumption, which is:\n",
    "\n",
    "The higher the Hamming weight of a word the higher the power consumption.\n",
    "\n",
    "Or more precise:\n",
    "\n",
    "**The power consumption of a device is proportional to the Hamming weight of the data it processes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "import lascar\n",
    "import numpy as np\n",
    "import plotly.graph_objects as pgo\n",
    "\n",
    "from securec.capture import capture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 3px solid plum; border-radius: 5px; padding: 5px; width: calc(100% - 20px);\">\n",
    "<div class=\"h2\" style=\"font-variant: all-small-caps;\">Exercise 1</div>\n",
    "\n",
    "Visualize the statement about linearity (=proportionality):\n",
    "\n",
    "1. Capture 500 traces with random input in the first byte.\n",
    "2. Group the traces by the Hamming weight of first input byte.\n",
    "3. Plot the 500 traces in one graph.\n",
    "4. Identify (by visual inspection) a sample point where the traces differ.\n",
    "5. Create a plot \"hamming(input) vs. trace[point]\". \n",
    "   I.e. put the values hw(attempt) on the x-axis and the mean value of the corresponding traces at the proper point on the y-axis.\n",
    "6. Think about to which instruction(s) the trace-point belongs to.\n",
    "\n",
    "Hints:\n",
    "- Use `fromfile=os.path.abspath(\"../lecture_3/sbox_lookup.c\")`.\n",
    "- If the plot does not look _linear_ look again for a better sweat spot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pearson correlation coefficient\n",
    "\n",
    "An interesting statistical formula to face this problem is given by the *Pearson correlation coefficient*. For two random variables $X, Y$ it is defined as\n",
    "\n",
    "$$\\rho_{X,Y} := \\frac{\\mathrm{Cov}(X, Y)}{\\sqrt{\\mathrm{Var}(X)} \\sqrt{\\mathrm{Var}(Y)}} \\ \\in [-1, 1]\\,.$$\n",
    "\n",
    "For two samples of finite length $x = {x_1, ..., x_n}$, $y = {y_1, ..., y_n}$ it can be defined as \n",
    "\n",
    "$$r_{x,y} := \\frac{\\sum_{i=1}^n (x_i - \\bar x)(y_i - \\bar y)}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar x)^2}\\sqrt{\\sum_{i=1}^n (y_i - \\bar y)^2}} \\ \\in [-1, 1]\\,,$$\n",
    "\n",
    "where $\\bar x := \\frac{1}{n} \\sum_{i=1}^n x_i$ is the mean of a sample $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 3px solid plum; border-radius: 5px; padding: 5px; width: calc(100% - 20px);\">\n",
    "<div class=\"h2\" style=\"font-variant: all-small-caps;\">Exercise 2</div>\n",
    "\n",
    "1. Implement Pearson correlation coefficient in python.\n",
    "2. Test your implementation against the following (x, y)-data with `size = 50`:\n",
    "    1. `(range(size), 5 * np.array(range(size)) + np.random.uniform(-size/4, size/4, size=size))` => $r \\approx 0.99$\n",
    "    2. `(range(size), np.array(range(size)) + np.random.uniform(-size, size, size=size) + 10)` => $r \\approx 0.32$\n",
    "    3. `(range(size), -3 * np.array(range(size)) + np.random.uniform(-size/4, size/4, size=size) + 200)` => $r \\approx -0.98$\n",
    "    4. `(range(size), 100 * np.sin(np.array(range(size)) / size * np.pi + np.pi) + np.random.uniform(-size/10, size/10, size=size))` => $r \\approx -0.05$\n",
    "3. Visualize above example data.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Attack!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to use this as principle for an attack?\n",
    "\n",
    "We saw that there is a _linear_ relationship between the trace at a given point and the Hamming weight of the input. Now we can quantify a linear dependency!\n",
    "\n",
    "So, we can develop an attack out of this principle:\n",
    "\n",
    "1. Record traces with random input.\n",
    "2. Focus on a point in the program where the input \"collides\" with the secret. In the case of an AES the output of the SBox lookup is a good point.\n",
    "3. For each input compute a \"forecast\" of hamming weights. If we do this for all possible secrets we will see that there is one guess where the Pearson coefficient between the forecast and the traces at this certain point maximizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 3px solid plum; border-radius: 5px; padding: 5px; width: calc(100% - 20px);\">\n",
    "<div class=\"h2\" style=\"font-variant: all-small-caps;\">Exercise 3</div>\n",
    "\n",
    "Implement an automated attack using Correlation Power Analysis, i.e. implement a function with the following signature:\n",
    "\n",
    "```python\n",
    "def aes_sbox_cpa(\n",
    "    traces,\n",
    "    key_byte_index=0,\n",
    "    trace_point=42,\n",
    "):\n",
    "```\n",
    "\n",
    "Hints:\n",
    "- Use a fixed sample point in the traces. We will see later how to get rid of this manual step.\n",
    "- It is not the Pearson coefficient which is important, but it's _absolute_ value.\n",
    "- Numpy offers great possibilities to index more dimensional arrays: You can write `data[\"trace\"][:, 42]` to get the 42-th sample point for each recorded trace.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 3px solid plum; border-radius: 5px; padding: 5px; width: calc(100% - 20px);\">\n",
    "<div class=\"h2\" style=\"font-variant: all-small-caps;\">Exercise 4</div>\n",
    "\n",
    "Perform a CPA attack using Lascar's `CpaEngine`.\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('chipwhisperer-td5n7f84')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "36760b1de54f02795c2144c8e36ca6fc5439200debad8ef46d119beb88a8f8a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
